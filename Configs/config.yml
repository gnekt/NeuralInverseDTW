# Cut a 192 frame T
# Questo modello usa norm dopo la somma con residual, in piu prima del positional encoding usa normalization.

log_dir: "Models/iDTW"
save_freq: 1
device: "cuda:1"
epochs: 5000
batch_size: 5
pretrained_model:
  path: ""
  what_to_load:
    encoder_prenet: True
    decoder_prenet: True
    eencoder: True
    encoder_norm: True
    decoder: True
    decoder_norm: True
    depost_interface: True
    postnet: False
  layer_freeze_mode:
    stage_1: # The number indicates layers, starting from bottom, that have to be freezed
      encoder_prenet: 1
      encoder: 6
      decoder_prenet: 1
      decoder: 6
      decpost_interface: 1
      postnet: 0
    stage_2: # The number indicates layers, starting from bottom, that have to be freezed
      encoder_prenet: 1
      encoder: 6
      decoder_prenet: 1
      decoder: 6
      decpost_interface: 1
      postnet: 0
    stage_3: # The number indicates layers, starting from bottom, that have to be freezed
      encoder_prenet: 1
      encoder: 6
      decoder_prenet: 1
      decoder: 6
      decpost_interface: 1
      postnet: 0
    stage_4: # The number indicates layers, starting from bottom, that have to be freezed
      encoder_prenet: 1
      encoder: 6
      decoder_prenet: 1
      decoder: 6
      decpost_interface: 1
      postnet: 0
    stage_5: # The number indicates layers, starting from bottom, that have to be freezed
      encoder_prenet: 1
      encoder: 6
      decoder_prenet: 1
      decoder: 2
      decpost_interface: 1
      postnet: 0
    stage_6: # The number indicates layers, starting from bottom, that have to be freezed
      encoder_prenet: 1
      encoder: 8
      decoder_prenet: 0
      decpost_interface: 1
      decoder: 0
      postnet: 0
    stage_7: # The number indicates layers, starting from bottom, that have to be freezed
      encoder_prenet: 0
      encoder: 0
      decoder_prenet: 0
      decpost_interface: 1
      decoder: 0
      postnet: 0
  freezing_mode: "Forever" # String -> Could be: Forever / Number of training epoch, Ex. "10"
    
tmux: 10

dataset_maker:
  dataset_output_dir: "Data/"

dataset_configuration:
  do_dtw: True
  data_separetor: "|"
  data_header: ["source_path","reference_path","reference_emotion"]
  training_set_path: "Data/eng_happy_training_list.txt"
  validation_set_path: "Data/eng_happy_validation_list.txt"

preprocess_params:
  sr: 24000
  spect_params:
    n_fft: 2048
    win_length: 1200
    hop_length: 300
    mel_fmin: 0.0
    mel_fmax: 8000.0
    n_mel_band: 80

training_parameter: 
  learning_rate: 0.00005
  warmp_up:
    warm_up_step: 12000
    noam_factor: 256
    
model_architecture:

  encoder_pre_net:
    activate: False
    conv_type: "Linear"
    input_size: 80
    output_size: 512
    layers_configuration: [[80,256,"tanh","tanh","",0.1],[256,512,"tanh","tanh","",0]] # [InChannels, OutChannels, KernelSize, PaddingSize, WeightInitialization] x NConvLayer

  encoder:
    device: "cuda:1"
    input_size: 80
    output_size: 80
    layers_configuration: [["normal",0]] # [LayerType, DropoutProb], LayerType -> (normal, residual)
    n_heads: 16
    n_attention_layer: 1
    conv1d_ff:
      input_size: 80
      output_size: 80
      layers_configuration: [[80,512,3,1,"linear","lrelu","norm",0],[512,80,3,1,"linear","lrelu","normd",.0]] # [InChannels, OutChannels, KernelSize, PaddingSize] x NConvLayer
    mlp_ff:
      input_size: 80
      output_size: 80
      layers_configuration: [[80,512,"linear","lrelu","",0],[512,80,"linear","lrelu","",.0]] # [InChannels, OutChannels, KernelSize, PaddingSize] x NConvLayer
    dropout: .3

  decoder_pre_net:
    activate: False
    conv_type: "Linear"
    input_size: 80
    output_size: 512
    layers_configuration: [[80,256,"tanh","tanh","",0.1],[256,512,"tanh","tanh","",.0]] # [InChannels, OutChannels, KernelSize, PaddingSize, WeightInitialization] x NConvLayer

  decoder:
    device: "cuda:1"
    architecture: "Custom" # else Custom
    input_size: 80
    output_size: 80
    decoder_postnet_interface_activate: True
    decoder_postnet_interface_dropout: 0.1
    decoder_postnet_interface_size: 80
    decoder_postnet_interface_activation: ""
    layers_configuration: [["normal",0]] # [LayerType, DropoutProb], LayerType -> (normal, residual)
    n_self_heads: 16
    n_self_attention_layer: 1
    n_encdec_heads: 16
    n_encdec_attention_layer: 3
    conv1d_ff:
      input_size: 80
      output_size: 80
      layers_configuration: [[80,512,3,1,"linear","lrelu","",0],[2048,512,3,1,"linear","lrelu","",.0]] # [InChannels, OutChannels, KernelSize, PaddingSize] x NConvLayer
    mlp_ff:
      input_size: 80
      output_size: 80
      layers_configuration: [[80,512,"linear","lrelu","",0],[512,512,"linear","lrelu","",0],[512,80,"linear","lrelu","",.0]] # [InChannels, OutChannels, KernelSize, PaddingSize] x NConvLayer
    dropout: .3 # Internal Layer Dropout

  post_net:
    conv_type: "1D"
    activate: True
    is_skip: False
    input_size: 80
    output_size: 80
    layers_configuration: [[80,128,3,1,"linear","lrelu","norm",0],[128,256,3,1,"linear","lrelu","norm",0],[256,128,3,1,"linear","lrelu","norm",0],[128,80,3,1,"linear","linear","",0]] # [InChannels, OutChannels, KernelSize, PaddingSize] x NConvLayer
    dropout: .0