{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# dataset = pd.read_csv(\"dataset.csv\", sep=\";\")\n",
    "# dataset = dataset[dataset[\"lang\"]==\"eng\"]\n",
    "# dataset[\"augmentation\"] = \"\"\n",
    "# dataset.to_csv(\"dataset_eng.csv\", sep=\";\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv(\"dataset_eng.csv\", sep=\";\")\n",
    "\n",
    "def create_augmentation_slow_down(path: str, language: str):\n",
    "    y, sr = librosa.load(f\"../{path}\")\n",
    "    new_file_name = path[:-4].split(f\"/{language}\")[1]\n",
    "    y_slow = librosa.effects.time_stretch(y, rate=random.uniform(0.4, 0.6))\n",
    "    new_path = f\"../StarGANv2-EmotionalVC/dataset/{language}_syn{new_file_name}_aug_slowed.wav\"\n",
    "    sf.write(f\"../../StarGANv2-EmotionalVC/dataset/{language}_syn{new_file_name}_aug_slowed.wav\", y_slow, sr)\n",
    "    return new_path\n",
    "\n",
    "def create_augmetation_pitch_shift_up(path: str, language: str):\n",
    "    y, sr = librosa.load(f\"../{path}\")\n",
    "    new_file_name = path[:-4].split(f\"/{language}\")[1]\n",
    "    y_shift = librosa.effects.pitch_shift(y, sr=sr, n_steps=4)\n",
    "    new_path = f\"../StarGANv2-EmotionalVC/dataset/{language}_syn{new_file_name}_aug_pitch.wav\"\n",
    "    sf.write(f\"../../StarGANv2-EmotionalVC/dataset/{language}_syn{new_file_name}_aug_pitch.wav\", y_shift, sr)\n",
    "    return new_path\n",
    "\n",
    "def create_augmetation_pitch_shift_down(path: str, language: str):\n",
    "    y, sr = librosa.load(f\"../{path}\")\n",
    "    new_file_name = path[:-4].split(f\"/{language}\")[1]\n",
    "    y_shift = librosa.effects.pitch_shift(y, sr=sr, n_steps=-4)\n",
    "    new_path = f\"../StarGANv2-EmotionalVC/dataset/{language}_syn{new_file_name}_aug_pitch_2.wav\"\n",
    "    sf.write(f\"../../StarGANv2-EmotionalVC/dataset/{language}_syn{new_file_name}_aug_pitch_2.wav\", y_shift, sr)\n",
    "    return new_path\n",
    "\n",
    "def create_augmetation_noise(path, language, noise_factor=0.2):\n",
    "    y, sr = librosa.load(f\"../{path}\")\n",
    "    new_file_name = path[:-4].split(f\"/{language}\")[1]\n",
    "    noise = np.random.randn(len(y)) * random.uniform(0.01, 0.2)\n",
    "    new_path = f\"../StarGANv2-EmotionalVC/dataset/{language}_syn{new_file_name}_aug_noise.wav\"\n",
    "    sf.write(f\"../../StarGANv2-EmotionalVC/dataset/{language}_syn{new_file_name}_aug_noise.wav\", y+noise, sr)\n",
    "    return new_path\n",
    "\n",
    "def create_augmetation_volume_scale(path, language, scale_factor=0.8):\n",
    "    y, sr = librosa.load(f\"../{path}\")\n",
    "    new_file_name = path[:-4].split(f\"/{language}\")[1]\n",
    "    y_scale = y * scale_factor\n",
    "    new_path = f\"../StarGANv2-EmotionalVC/dataset/{language}_syn{new_file_name}_aug_volume_scale.wav\"\n",
    "    sf.write(f\"../../StarGANv2-EmotionalVC/dataset/{language}_syn{new_file_name}_aug_volume_scale.wav\", y_scale, sr)\n",
    "    return new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14202 [00:00<?, ?it/s]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 1/14202 [00:01<5:33:36,  1.41s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 2/14202 [00:02<4:22:45,  1.11s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 3/14202 [00:03<4:41:02,  1.19s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 4/14202 [00:05<5:06:57,  1.30s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 5/14202 [00:06<5:04:00,  1.28s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 6/14202 [00:07<4:19:31,  1.10s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 7/14202 [00:07<3:57:24,  1.00s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 8/14202 [00:09<4:31:53,  1.15s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 9/14202 [00:10<4:42:22,  1.19s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 10/14202 [00:12<5:18:34,  1.35s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 11/14202 [00:13<5:40:13,  1.44s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 12/14202 [00:15<5:39:46,  1.44s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 13/14202 [00:17<6:20:20,  1.61s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 14/14202 [00:19<7:18:04,  1.85s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 15/14202 [00:21<6:43:09,  1.71s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 16/14202 [00:23<7:14:13,  1.84s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 17/14202 [00:24<6:30:20,  1.65s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 18/14202 [00:25<5:34:55,  1.42s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 19/14202 [00:26<4:58:46,  1.26s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 20/14202 [00:27<4:49:14,  1.22s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 21/14202 [00:28<4:41:09,  1.19s/it]/tmp/ipykernel_8892/1811476441.py:18: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
      "  0%|          | 22/14202 [00:29<4:54:20,  1.25s/it]"
     ]
    }
   ],
   "source": [
    "\n",
    "for index, row in tqdm(dataset.iterrows(), total=dataset.shape[0]):\n",
    "    _row_slow = copy.deepcopy(row)\n",
    "    _row_slow[\"path\"] = create_augmentation_slow_down(_row_slow[\"path\"], _row_slow[\"lang\"])\n",
    "    _row_slow[\"augmentation\"] = \"slow_down\"\n",
    "    \n",
    "    _row_pitch = copy.deepcopy(row)\n",
    "    _row_pitch[\"path\"] = create_augmetation_pitch_shift_up(_row_pitch[\"path\"], _row_pitch[\"lang\"])\n",
    "    _row_pitch[\"augmentation\"] = \"pitch_shift\"\n",
    "    \n",
    "    _row_pitch_2 = copy.deepcopy(row)\n",
    "    _row_pitch_2[\"path\"] = create_augmetation_pitch_shift_down(_row_pitch_2[\"path\"], _row_pitch_2[\"lang\"])\n",
    "    _row_pitch_2[\"augmentation\"] = \"pitch_shift_2\"\n",
    "    \n",
    "    _row_noise = copy.deepcopy(row)\n",
    "    _row_noise[\"path\"] = create_augmetation_noise(_row_noise[\"path\"], _row_noise[\"lang\"])\n",
    "    _row_noise[\"augmentation\"] = \"noise\"\n",
    "    \n",
    "    dataset = dataset.append([_row_pitch, _row_noise, _row_pitch, _row_pitch_2, _row_slow])\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset\u001b[39m.\u001b[39mreset_index()\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39m./dataset_eng_with_aug.csv\u001b[39m\u001b[39m\"\u001b[39m,sep\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m;\u001b[39m\u001b[39m\"\u001b[39m,index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,columns\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mdataset\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mlang\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mpath\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mactor_id\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mgender\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39memotion\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mstatement_id\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39maugmentation\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset.reset_index().to_csv(\"./dataset_eng_with_aug.csv\",sep=\";\",index=False,columns=[\"dataset\",\"lang\",\"path\",\"actor_id\",\"gender\",\"emotion\",\"statement_id\",\"augmentation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_aug=dataset_aug.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_aug = pd.read_csv(\"dataset_with_aug.csv\", sep=\";\")\n",
    "dataset_happy = dataset_aug[(dataset_aug[\"emotion\"]==\"happy\") | (dataset_aug[\"emotion\"]==\"neutral\")]\n",
    "\n",
    "preparing_dataset=[]\n",
    "for index, group in dataset_happy.groupby([\"dataset\",\"actor_id\",\"statement_id\"]):\n",
    "    try:\n",
    "        neutral = group[group[\"emotion\"] == \"neutral\"].iloc[0]\n",
    "    except Exception as ex:\n",
    "        continue\n",
    "    for _nested_index, _nested_row in group[group[\"emotion\"] != \"neutral\"].iterrows():\n",
    "        if neutral[\"dataset\"]!=_nested_row[\"dataset\"]:\n",
    "            print(\"\")\n",
    "        preparing_dataset.append([neutral[\"path\"], _nested_row[\"path\"]])\n",
    "\n",
    "preparing_dataset = pd.DataFrame(preparing_dataset).sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "################################################\n",
    "config_path=\"../Configs/config.yml\"\n",
    "config = yaml.safe_load(open(config_path))\n",
    "\n",
    "training_set_percentage = config.get('training_set_percentage', 80)\n",
    "validation_set_percentage = config.get('validation_set_percentage', 20)\n",
    "\n",
    "training_path = config.get('training_path', \"../Data/eng_aug_happy_training_list.txt\")\n",
    "validation_path = config.get('validation_path', \"../Data/eng_aug_happy_validation_list.txt\" )\n",
    "################################################\n",
    "preparing_dataset = pd.DataFrame(preparing_dataset).sample(frac=1)\n",
    "# Define stream \n",
    "# pick sample for training and validation\n",
    "training_dataframe = preparing_dataset.iloc[0:int((preparing_dataset.shape[0]*training_set_percentage)/100)]\n",
    "validation_dataframe = preparing_dataset.iloc[preparing_dataset.shape[0]-int((preparing_dataset.shape[0]*validation_set_percentage)/100):]\n",
    "\n",
    "training_file = open(training_path,\"w\")\n",
    "# Create training file\n",
    "for index, row in training_dataframe.iterrows():\n",
    "    try:\n",
    "        training_file.write(f\"{row[0]}|{row[1]}\\n\")\n",
    "    except IOError as e:\n",
    "        print(e)\n",
    "training_file.close()\n",
    "\n",
    "\n",
    "validation_file = open(validation_path,\"w\")\n",
    "# Create validation file\n",
    "for index, row in validation_dataframe.iterrows():\n",
    "    try:\n",
    "        validation_file.write(f\"{row[0]}|{row[1]}\\n\")\n",
    "    except IOError as e:\n",
    "        print(e)\n",
    "validation_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "stargan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af3ab013e161d7e2071e405d816695dd1dca74efb3c18ca57ddcf9213a3f1219"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
